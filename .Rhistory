actual_globalthresholds_corr_df$estimate <- format(round(actual_globalthresholds_corr_df$estimate, 3), nsmall = 2)
actual_globalthresholds_corr_df <- actual_globalthresholds_corr_df %>% dplyr::select(threshold, p.value, estimate)
ggplot(actual_globalthresholds_corr_df, aes(x = threshold, y = as.numeric(estimate))) +
geom_point() +
#ylim(0, 1) +
theme_bw()
actual_globalthresholds_corr_df <-actual_globalthresholds_corr_df %>%
mutate(data_type = "actual")
################################## INITIAL THRESHOLD DATASET: TARGET FORMS ###########################
# Do all the same again for Target forms
globaldistance_lyon_target <- feather::read_feather("Data/globaldistance_lyon.feather") %>% filter(data_type == "target")
thresholds <- seq(from = 0, to = 1, by = 0.01)  # create empty list
names(thresholds) <- paste("threshold", thresholds, sep ="_") # name list object
target_globaldistance_lyon_list <- lapply(thresholds, function(t) {
filter(globaldistance_lyon_target, distance_norm < t) %>%
group_by(Speaker, age, gloss1) %>%                     # for gloss1 side of the data, otherwise 50% of data is missed (oops)
tally()
})
target_globaldistance_lyon_list_melted <- melt(target_globaldistance_lyon_list)
#target_globaldistance_lyon_list_melted %>% filter(Speaker == "Alex" & age == 17) %>% distinct(gloss1)
target_globalthresholds_lyon_base <- target_globaldistance_lyon_list_melted %>%
rename("degree" = "value") %>%
separate(L1, c("l1", "threshold"), sep = "_") %>%
dplyr::select(-l1, -variable) %>%
mutate(data_type = "target")
target_globalthresholds_l <- target_globalthresholds_lyon_base %>%
left_join(first_instance_base_Lyon) %>%
distinct(Speaker, AOP, gloss1, threshold, .keep_all = TRUE)
target_globalthresholds_lyon <- target_globalthresholds_lyon_base %>%
left_join(first_instance_base_Lyon)
target_globalthresholds_AOP_lyon <- target_globalthresholds_lyon_base %>%
left_join(first_instance_base_Lyon)  %>%
group_by(Speaker) %>%
filter(age == max(age))
##### target GLOBAL NETWORK: degree ~ AOP correlations across thresholds ########
threshold_names <- seq(0.01, 1, by = 0.01)
thresholds_corr <- vector("list", length(101)) #Prep a list to store your corr.test results
names <- names(threshold_names)
counter = 0 # To store your corr.test into list through iterating
for (i in unique(target_globalthresholds_AOP_lyon$threshold)){
counter = counter + 1
# Creating new variables makes the code clearer
x = as.numeric(target_globalthresholds_AOP_lyon[target_globalthresholds_AOP_lyon$threshold == i,]$degree)
y = as.numeric(target_globalthresholds_AOP_lyon[target_globalthresholds_AOP_lyon$threshold == i,]$AOP)
thresholds_corr[[counter]] <-cor.test(x,y,method="spearman")
}
target_globalthresholds_corr <- setNames(thresholds_corr, paste0("threshold", threshold_names))
target_globalthresholds_corr_df <- data.frame(t(sapply(target_globalthresholds_corr,c)))
target_globalthresholds_corr_df <- target_globalthresholds_corr_df %>%
tibble::rownames_to_column(var = "threshold")
target_globalthresholds_corr_df$threshold <-gsub("[a-zA-Z]", "", target_globalthresholds_corr_df$threshold)
target_globalthresholds_corr_df$estimate <-gsub("[c(rho = )]", "", target_globalthresholds_corr_df$estimate)
target_globalthresholds_corr_df$threshold <-as.numeric(target_globalthresholds_corr_df$threshold)
target_globalthresholds_corr_df$estimate <-as.numeric(target_globalthresholds_corr_df$estimate)
target_globalthresholds_corr_df$p.value <-as.numeric(target_globalthresholds_corr_df$p.value)
target_globalthresholds_corr_df$p.value <- format(round(target_globalthresholds_corr_df$p.value, 2), nsmall = 2)
target_globalthresholds_corr_df$estimate <- format(round(target_globalthresholds_corr_df$estimate, 3), nsmall = 2)
target_globalthresholds_corr_df <- target_globalthresholds_corr_df %>% dplyr::select(threshold, p.value, estimate)
ggplot(target_globalthresholds_corr_df, aes(x = threshold, y = as.numeric(estimate))) +
geom_point() +
#ylim(0, 1) +
theme_bw()
target_globalthresholds_corr_df <-target_globalthresholds_corr_df %>%
mutate(data_type = "target")
##################################
globalthresholds_corr_lyon <- rbind(target_globalthresholds_corr_df, actual_globalthresholds_corr_df)
globalthresholds_lyon <- rbind(target_globalthresholds_lyon, actual_globalthresholds_lyon)
globalthresholds_AOP_lyon <- rbind(target_globalthresholds_AOP_lyon, actual_globalthresholds_AOP_lyon) %>%
mutate(corpus = "French")
feather::write_feather(globalthresholds_corr_lyon, "Data/globalthresholds_corr_lyon.feather") # correlation output data
feather::write_feather(globalthresholds_lyon, "Data/globalthresholds_lyon.feather") # all types at all ages, plus AOP data
#feather::write_feather(globaldistance, "Data/large_files/globaldistance.feather") # distance between each word pair at each age
feather::write_feather(globalthresholds_AOP_lyon, "Data/globalthresholds_AOP_lyon.feather") # AOP for full network, taken at last month of data (30 months)
# Updated 24th February 2023
# This script prepares the data for analysis from each of the global distance matrices
# It calculates the degree for each word pair across a set of thresholds from E=0.1-1 in relation to age, vocab size, and age of production (global network)
# It then runs correlations between degree and AOP
#source("prelims.R")
globaldistance_Providence_actual <- feather::read_feather("Data/globaldistance_Providence.feather") %>% filter(data_type == "actual")
################################## INITIAL THRESHOLD DATASET: ACTUAL FORMS ###########################
# Create a new dataframe to show degree of connectivity for a range of thresholds between 0 and 1, as per Amatuni & Bergelson, 2017
thresholds <- seq(from = 0, to = 1, by = 0.01)  # create empty list
names(thresholds) <- paste("threshold", thresholds, sep ="_") # name list object
globaldistance_list_providence_actual <- lapply(thresholds, function(t) {
filter(globaldistance_Providence_actual, distance_norm < t) %>%
group_by(Speaker, age, gloss1) %>%                     # for gloss1 side of the data, otherwise 50% of data is missed (oops)
tally()
})
globaldistance_list_providence_actual_melted <- melt(globaldistance_list_providence_actual)
globaldistance_list_providence_actual_degree <- globaldistance_list_providence_actual_melted %>%
rename("degree" = "value") %>%
separate(L1, into = c("remove", "threshold"), sep = "_") %>%
filter(threshold == 0.25) %>%
dplyr::select(-remove, -threshold, -variable)
#feather::write_feather(globaldistance_list_providence_actual_degree, "Data/actual_globaldistance_list_degree_providence.feather")
#globaldistance_list_providence_actual_degree <- feather::read_feather("Data/actual_globaldistance_list_degree_providence.feather")
#globaldistance_list_providence_actual_degree %>% filter(Speaker == "Alex" & age == 16) %>% distinct(gloss1)
actual_globalthresholds_base_providence <- globaldistance_list_providence_actual_melted %>%
rename("degree" = "value") %>%
separate(L1, c("l1", "threshold"), sep = "_") %>%
dplyr::select(-l1, -variable) %>%
mutate(data_type = "actual",
age = as.numeric(age)) %>%
feather::write_feather("Data/actual_globalthresholds_base_providence.feather")
first_instance_base <- read_csv("Data/first_instance_Providence.csv")
actual_globalthresholds_providence <- actual_globalthresholds_base_providence %>%
filter(threshold == .25) %>%
left_join(first_instance_base)
actual_globalthresholds_AOP_providence <- actual_globalthresholds_base_providence %>%
#filter(threshold == .25) %>%
left_join(first_instance_base)  %>%
group_by(Speaker) %>%
filter(age == max(age))
##### ACTUAL GLOBAL NETWORK: degree ~ AOP correlations across thresholds ########
threshold_names <- seq(0.01, 1, by = 0.01)
thresholds_corr <- vector("list", length(101)) #Prep a list to store your corr.test results
names <- names(threshold_names)
counter = 0 # To store your corr.test into list through iterating
for (i in unique(actual_globalthresholds_AOP_providence$threshold)){
counter = counter + 1
# Creating new variables makes the code clearer
x = as.numeric(actual_globalthresholds_AOP_providence[actual_globalthresholds_AOP_providence$threshold == i,]$degree)
y = as.numeric(actual_globalthresholds_AOP_providence[actual_globalthresholds_AOP_providence$threshold == i,]$AOP)
thresholds_corr[[counter]] <-cor.test(x,y,method="spearman")
}
actual_globalthresholds_corr <- setNames(thresholds_corr, paste0("threshold", threshold_names))
actual_globalthresholds_corr_df <- data.frame(t(sapply(actual_globalthresholds_corr,c)))
actual_globalthresholds_corr_df <- actual_globalthresholds_corr_df %>%
tibble::rownames_to_column(var = "threshold")
actual_globalthresholds_corr_df$threshold <-gsub("[a-zA-Z]", "", actual_globalthresholds_corr_df$threshold)
actual_globalthresholds_corr_df$estimate <-gsub("[c(rho = )]", "", actual_globalthresholds_corr_df$estimate)
actual_globalthresholds_corr_df$threshold <-as.numeric(actual_globalthresholds_corr_df$threshold)
actual_globalthresholds_corr_df$estimate <-as.numeric(actual_globalthresholds_corr_df$estimate)
actual_globalthresholds_corr_df$p.value <-as.numeric(actual_globalthresholds_corr_df$p.value)
actual_globalthresholds_corr_df$p.value <- format(round(actual_globalthresholds_corr_df$p.value, 2), nsmall = 2)
actual_globalthresholds_corr_df$estimate <- format(round(actual_globalthresholds_corr_df$estimate, 3), nsmall = 2)
actual_globalthresholds_corr_df <- actual_globalthresholds_corr_df %>% dplyr::select(threshold, p.value, estimate)
# ggplot(actual_globalthresholds_corr_df, aes(x = threshold, y = as.numeric(estimate))) +
#   geom_point() +
#   #ylim(0, 1) +
#   theme_bw()
actual_globalthresholds_corr_df <-actual_globalthresholds_corr_df %>%
mutate(data_type = "actual")
################################## INITIAL THRESHOLD DATASET: TARGET FORMS ###########################
# Do all the same again for Target forms
globaldistance_Providence_target <- feather::read_feather("Data/globaldistance_Providence.feather") %>% filter(data_type == "target")
thresholds <- seq(from = 0, to = 1, by = 0.01)  # create empty list
names(thresholds) <- paste("threshold", thresholds, sep ="_") # name list object
target_globaldistance_providence_list <- lapply(thresholds, function(t) {
filter(globaldistance_Providence_target, distance_norm < t) %>%
group_by(Speaker, age, gloss1) %>%                     # for gloss1 side of the data, otherwise 50% of data is missed (oops)
tally()
})
target_globaldistance_providence_list_melted <- melt(target_globaldistance_providence_list)
target_globalthresholds_providence_base <- target_globaldistance_providence_list_melted %>%
rename("degree" = "value") %>%
separate(L1, c("l1", "threshold"), sep = "_") %>%
dplyr::select(-l1, -variable) %>%
mutate(data_type = "target",
age = as.numeric(age))
target_globalthresholds_providence <- target_globalthresholds_providence_base %>%
filter(threshold == .25) %>%
left_join(first_instance_base) #%>%
#distinct(Speaker, AOP, gloss1, threshold, .keep_all = TRUE)
target_globalthresholds_AOP_providence <- target_globalthresholds_providence_base %>%
left_join(first_instance_base)  %>%
group_by(Speaker) %>%
filter(age == max(age))
##### target GLOBAL NETWORK: degree ~ AOP correlations across thresholds ########
threshold_names <- seq(0.01, 1, by = 0.01)
thresholds_corr <- vector("list", length(101)) #Prep a list to store your corr.test results
names <- names(threshold_names)
counter = 0 # To store your corr.test into list through iterating
for (i in unique(target_globalthresholds_AOP_providence$threshold)){
counter = counter + 1
# Creating new variables makes the code clearer
x = as.numeric(target_globalthresholds_AOP_providence[target_globalthresholds_AOP_providence$threshold == i,]$degree)
y = as.numeric(target_globalthresholds_AOP_providence[target_globalthresholds_AOP_providence$threshold == i,]$AOP)
thresholds_corr[[counter]] <-cor.test(x,y,method="spearman")
}
target_globalthresholds_corr <- setNames(thresholds_corr, paste0("threshold", threshold_names))
target_globalthresholds_corr_df <- data.frame(t(sapply(target_globalthresholds_corr,c)))
target_globalthresholds_corr_df <- target_globalthresholds_corr_df %>%
tibble::rownames_to_column(var = "threshold")
target_globalthresholds_corr_df$threshold <-gsub("[a-zA-Z]", "", target_globalthresholds_corr_df$threshold)
target_globalthresholds_corr_df$estimate <-gsub("[c(rho = )]", "", target_globalthresholds_corr_df$estimate)
target_globalthresholds_corr_df$threshold <-as.numeric(target_globalthresholds_corr_df$threshold)
target_globalthresholds_corr_df$estimate <-as.numeric(target_globalthresholds_corr_df$estimate)
target_globalthresholds_corr_df$p.value <-as.numeric(target_globalthresholds_corr_df$p.value)
target_globalthresholds_corr_df$p.value <- format(round(target_globalthresholds_corr_df$p.value, 2), nsmall = 2)
target_globalthresholds_corr_df$estimate <- format(round(target_globalthresholds_corr_df$estimate, 3), nsmall = 2)
target_globalthresholds_corr_df <- target_globalthresholds_corr_df %>% dplyr::select(threshold, p.value, estimate)
ggplot(target_globalthresholds_corr_df, aes(x = threshold, y = as.numeric(estimate))) +
geom_point() +
#ylim(0, 1) +
theme_bw()
target_globalthresholds_corr_df <-target_globalthresholds_corr_df %>%
mutate(data_type = "target")
##################################
globalthresholds_corr_providence <- rbind(target_globalthresholds_corr_df, actual_globalthresholds_corr_df)
globalthresholds_providence <- rbind(target_globalthresholds_providence, actual_globalthresholds_providence)
globalthresholds_AOP_providence <- rbind(target_globalthresholds_AOP_providence, actual_globalthresholds_AOP_providence) %>%
mutate(corpus = "English")
feather::write_feather(globalthresholds_corr_providence, "Data/globalthresholds_corr_providence.feather") # correlation output data
feather::write_feather(globalthresholds_providence, "Data/globalthresholds_providence.feather") # all types at all ages, plus AOP data
#feather::write_feather(globaldistance, "Data/globaldistance.feather") # distance between each word pair at each age
feather::write_feather(globalthresholds_AOP_providence, "Data/globalthresholds_AOP_providence.feather") # AOP for full network, taken at last month of data (30 months)
# Updated 9th March 2023
global_distance_providence <- feather::read_feather("Data/globaldistance_Providence.feather")
# global_distance_providence %>%
#   filter(Speaker == "Violet" & (gloss1 == "medicine" | gloss2 == "medicine")) %>%
#   group_by(age) %>% tally()
globalthresholds_providence <- feather::read_feather("Data/globalthresholds_providence.feather")
# globalthresholds_providence %>%
#   filter(Speaker == "Violet" & gloss1 == "medicine") %>%
#   group_by(age) %>% tally()
globalthresholds_AOP_providence <- feather::read_feather("Data/globalthresholds_AOP_providence.feather") %>%
filter(threshold == 0.25)
vocabsize_sub <- vocabsize_providence %>% distinct(Speaker, AOP, vocab_month)
mean_degree_full_target <- feather::read_feather("Data/mean_degree_full_target_providence.feather")
mean_degree_full_actual <- feather::read_feather("Data/mean_degree_full_actual_providence.feather")
global_network <- globalthresholds_AOP_providence %>%
rename("PAQ_val" = "degree") %>%
dplyr::select(-age, -threshold)
mean_degree_full <- rbind(mean_degree_full_actual, mean_degree_full_target)
comparison_data <- read_csv("Data/comparison_data_providence.csv") %>%
distinct(Gloss, Speaker, .keep_all=T) %>%
dplyr::select(Gloss, Speaker, Targetphon, nsyl_target) %>%
rename("gloss1" = "Gloss")
global_network_split <- global_network %>%
pivot_wider(names_from = data_type, values_from = PAQ_val) %>%
rename("PAQ_target" = "target",
"PAQ_actual" = "actual")
regression_data <- mean_degree_full %>% left_join(global_network_split) %>%
group_by(Speaker, gloss1, data_type) %>%
mutate(learned_next = ifelse(age == AOP-1, 1, 0)) %>%
filter(age != AOP) %>%
left_join(comparison_data) %>%
left_join(vocabsize_providence) %>%
ungroup() %>%
mutate(AOP_scaled = c(scale(AOP, center = TRUE, scale = TRUE)),
length_scaled = c(scale(Targetphon, center = TRUE, scale = TRUE)),
PAT_weighted = PAT_val/vocab_agg,
PAQ_weighted = PAQ_target/vocab_agg) %>%
group_by(Speaker) %>%
mutate(PAT_scaled = c(scale(PAT_val, center = TRUE, scale = TRUE)),
PAT_scaled_m = c(scale(PAT_val_m, center = TRUE, scale = TRUE)),
PAT_vocab_scaled = c(scale(PAT_weighted, center = TRUE, scale = TRUE)),
PAQ_vocab_scaled = c(scale(PAQ_weighted, center = T, scale = T)),
PAQ_scaled_target = c(scale(PAQ_target, center = TRUE, scale = TRUE)),
PAQ_scaled_actual = c(scale(PAQ_actual, center = TRUE, scale = TRUE)))
# install.packages("stringi")
# install.packages("ggplot2")
# install.packages("stringr")
# install.packages("tibble")
# install.packages("ggpubr")
# install.packages("lmerTest")
# install.packages("papaja")
# install.packages("tidyverse")
# install.packages("dplyr")
# install.packages("tibble")
# install.packages("afex")
# install.packages("citr")
# install.packages("glmmTMB")
# install.packages("janitor")
#install.packages("linguisticsdown")
#install.packages("ggraph")
#install.packages("wesanderson")
# install.packages("effects")
# install.packages("reshape2")
library(tidyverse)
library(stringi)
library(stringr)
library(ggplot2)
library(tibble)
library(lmerTest)
library(papaja)
library(tidyverse)
library(dplyr)
#library(citr)
library(feather)
library(ggthemes)
#library(effects)
library(nlme)
library(glmmTMB)
library(broom)
library(kableExtra)
library(knitr)
library(interactions)
library(janitor)
library(ggraph)
library(data.table)
library(linguisticsdown)
library(ggridges)
library(wesanderson)
library(igraph)
library(reshape2)
library(tidyr)
options("encoding" = "UTF-8")
regression_data <- mean_degree_full %>% left_join(global_network_split) %>%
group_by(Speaker, gloss1, data_type) %>%
mutate(learned_next = ifelse(age == AOP-1, 1, 0)) %>%
filter(age != AOP) %>%
left_join(comparison_data) %>%
left_join(vocabsize_providence) %>%
ungroup() %>%
mutate(AOP_scaled = c(scale(AOP, center = TRUE, scale = TRUE)),
length_scaled = c(scale(Targetphon, center = TRUE, scale = TRUE)),
PAT_weighted = PAT_val/vocab_agg,
PAQ_weighted = PAQ_target/vocab_agg) %>%
group_by(Speaker) %>%
mutate(PAT_scaled = c(scale(PAT_val, center = TRUE, scale = TRUE)),
PAT_scaled_m = c(scale(PAT_val_m, center = TRUE, scale = TRUE)),
PAT_vocab_scaled = c(scale(PAT_weighted, center = TRUE, scale = TRUE)),
PAQ_vocab_scaled = c(scale(PAQ_weighted, center = T, scale = T)),
PAQ_scaled_target = c(scale(PAQ_target, center = TRUE, scale = TRUE)),
PAQ_scaled_actual = c(scale(PAQ_actual, center = TRUE, scale = TRUE)))
global_network <- globalthresholds_AOP_providence %>%
rename("PAQ_val" = "degree") %>%
dplyr::select(-age, -threshold)
global_distance_providence <- feather::read_feather("Data/globaldistance_Providence.feather")
# global_distance_providence %>%
#   filter(Speaker == "Violet" & (gloss1 == "medicine" | gloss2 == "medicine")) %>%
#   group_by(age) %>% tally()
globalthresholds_providence <- feather::read_feather("Data/globalthresholds_providence.feather")
# globalthresholds_providence %>%
#   filter(Speaker == "Violet" & gloss1 == "medicine") %>%
#   group_by(age) %>% tally()
globalthresholds_AOP_providence <- feather::read_feather("Data/globalthresholds_AOP_providence.feather") %>%
filter(threshold == 0.25)
# globalthresholds_AOP_providence %>%
#   filter(Speaker == "Violet" & gloss1 == "medicine") %>%
#   group_by(age) %>% tally()
vocabsize_providence <- feather::read_feather("Data/globalthresholds_AOP_providence.feather") %>%
filter(threshold == 0.99, data_type == "target") %>%  # use 0.99 as threshold to make sure all new words are incorporated.
#filter(age == 30) %>%
group_by(Speaker, AOP) %>%
tally() %>%
rename("vocab_month" = "n") %>%
mutate(vocab_agg = cumsum(vocab_month))
vocabsize_sub <- vocabsize_providence %>% distinct(Speaker, AOP, vocab_month)
mean_degree_full_target <- feather::read_feather("Data/mean_degree_full_target_providence.feather")
mean_degree_full_actual <- feather::read_feather("Data/mean_degree_full_actual_providence.feather")
global_network <- globalthresholds_AOP_providence %>%
rename("PAQ_val" = "degree") %>%
dplyr::select(-age, -threshold)
mean_degree_full <- rbind(mean_degree_full_actual, mean_degree_full_target)
comparison_data <- read_csv("Data/comparison_data_providence.csv") %>%
distinct(Gloss, Speaker, .keep_all=T) %>%
dplyr::select(Gloss, Speaker, Targetphon, nsyl_target) %>%
rename("gloss1" = "Gloss")
global_network_split <- global_network %>%
pivot_wider(names_from = data_type, values_from = PAQ_val) %>%
rename("PAQ_target" = "target",
"PAQ_actual" = "actual")
regression_data <- mean_degree_full %>% left_join(global_network_split) %>%
group_by(Speaker, gloss1, data_type) %>%
mutate(learned_next = ifelse(age == AOP-1, 1, 0)) %>%
filter(age != AOP) %>%
left_join(comparison_data) %>%
left_join(vocabsize_providence) %>%
ungroup() %>%
mutate(AOP_scaled = c(scale(AOP, center = TRUE, scale = TRUE)),
length_scaled = c(scale(Targetphon, center = TRUE, scale = TRUE)),
PAT_weighted = PAT_val/vocab_agg,
PAQ_weighted = PAQ_target/vocab_agg) %>%
group_by(Speaker) %>%
mutate(PAT_scaled = c(scale(PAT_val, center = TRUE, scale = TRUE)),
PAT_scaled_m = c(scale(PAT_val_m, center = TRUE, scale = TRUE)),
PAT_vocab_scaled = c(scale(PAT_weighted, center = TRUE, scale = TRUE)),
PAQ_vocab_scaled = c(scale(PAQ_weighted, center = T, scale = T)),
PAQ_scaled_target = c(scale(PAQ_target, center = TRUE, scale = TRUE)),
PAQ_scaled_actual = c(scale(PAQ_actual, center = TRUE, scale = TRUE)))
chi_freq <- read_csv("Data/freq_providence.csv")
session_data <- read_csv("Data/comparison_data_providence.csv") %>%    # need to add ordinal session numbers for GAMMs
group_by(Speaker, age) %>%
tally() %>%
filter(n > 1) %>%
dplyr::select(Speaker, age) %>%
group_by(Speaker, age) %>%
tally() %>%
mutate(session_ordinal = row_number()) %>%
dplyr::select(-n)
word_cat <- feather::read_feather("Data/FULLsample_Providence.feather") %>%
distinct(Gloss, .keep_all = T) %>%
dplyr::select(Gloss, category) %>%
rename("gloss1" = "Gloss") %>%
mutate(category = as.factor(category),
category = fct_collapse(category,
object_word = c("animals", "body_parts", "clothing", "food_drink", "furniture_rooms",
"household", "people", "outside", "places", "toys", "vehicles"),
verbs = c("action_words", "helping_verbs")))
FULLsample_var <- feather::read_feather("Data/FULLsample_Providence.feather") %>%
group_by(Speaker, Gloss) %>%
tally() %>%
rename("gloss1" = "Gloss",
"n_tokens" = "n")   # how many tokens of each word included in the data
regression_data <- regression_data %>%
left_join(chi_freq) %>%
left_join(word_cat) %>%
left_join(FULLsample_var) %>%
left_join(session_data) %>%
mutate(total_freq = ifelse(is.na(total_freq), 0, total_freq)) %>%
mutate(freq_scaled = c(scale(total_freq, center = TRUE, scale = TRUE)),
vocab_scaled = c(scale(vocab_agg, center = TRUE, scale = TRUE)),
tokens_scaled = c(scale(n_tokens, center = TRUE, scale = TRUE))) %>%
mutate(corpus = "English",
age_scaled = c(scale(age, center = T, scale = T)),
category = as.factor(category),
data_type = as.factor(data_type),
Speaker = as.factor(Speaker))
feather::write_feather(regression_data, "Data/regression_data_providence.feather")
# need to start with full list of all words by age, global_network$gloss1
global_distance_lyon <- feather::read_feather("Data/globaldistance_lyon.feather")
globalthresholds_lyon <- feather::read_feather("Data/globalthresholds_lyon.feather")
globalthresholds_AOP_lyon <- feather::read_feather("Data/globalthresholds_AOP_lyon.feather") %>%
filter(threshold == 0.25)
vocabsize_lyon <- feather::read_feather("Data/globalthresholds_AOP_lyon.feather") %>%
filter(threshold == 0.99, data_type == "target") %>%  # use 0.99 as threshold to make sure all new words are incorporated.
#filter(age == 30) %>%
group_by(Speaker, AOP) %>%
tally() %>%
rename("vocab_month" = "n") %>%
mutate(vocab_agg = cumsum(vocab_month))
ages <- global_distance_lyon %>% filter(data_type == "target") %>% distinct(Speaker, gloss1, age)
AOP_summ_actual <- globalthresholds_AOP_lyon %>%
mutate(Speaker_AOP = paste(Speaker, AOP, sep="_"),
AOP = as.numeric((AOP))) %>%
filter(data_type == "actual")
AOP_summ_target <- globalthresholds_AOP_lyon %>%
mutate(Speaker_AOP = paste(Speaker, AOP, sep="_"),
AOP = as.numeric((AOP))) %>%
filter(data_type == "target")
AOP_summ <- rbind(AOP_summ_actual, AOP_summ_target) %>% distinct(Speaker, gloss1, .keep_all = T)
vocabsize_sub <- vocabsize_lyon %>% distinct(Speaker, AOP, vocab_month)
mean_degree_full_target <- feather::read_feather("Data/mean_degree_full_target_lyon.feather")
mean_degree_full_actual <- feather::read_feather("Data/mean_degree_full_actual_lyon.feather")
global_network <- globalthresholds_AOP_lyon %>%
rename("PAQ_val" = "degree") %>%
dplyr::select(-age, -threshold)
mean_degree_full <- rbind(mean_degree_full_actual, mean_degree_full_target)
comparison_data <- read_csv("Data/comparison_data_lyon.csv") %>%
distinct(Gloss, Speaker, .keep_all=T) %>%
dplyr::select(Gloss, Speaker, Targetphon, nsyl_target) %>%
rename("gloss1" = "Gloss")
global_network_split <- global_network %>%
pivot_wider(names_from = data_type, values_from = PAQ_val) %>%
rename("PAQ_target" = "target",
"PAQ_actual" = "actual")
# global_network_split %>% filter(is.na(PAQ_actual) | is.na(PAQ_target))  # 30 datapoints
regression_data <- mean_degree_full %>% left_join(global_network_split) %>%
group_by(Speaker, gloss1, data_type) %>%
mutate(learned_next = ifelse(age == AOP-1, 1, 0)) %>%
filter(age != AOP) %>%
left_join(comparison_data) %>%
left_join(vocabsize_lyon) %>%
ungroup() %>%
mutate(AOP_scaled = c(scale(AOP, center = TRUE, scale = TRUE)),
length_scaled = c(scale(Targetphon, center = TRUE, scale = TRUE)),
PAT_weighted = PAT_val/vocab_agg,
PAQ_weighted = PAQ_target/vocab_agg) %>%
group_by(Speaker) %>%
mutate(PAT_scaled = c(scale(PAT_val, center = TRUE, scale = TRUE)),
PAT_scaled_m = c(scale(PAT_val_m, center = TRUE, scale = TRUE)),
PAT_vocab_scaled = c(scale(PAT_weighted, center = TRUE, scale = TRUE)),
PAQ_vocab_scaled = c(scale(PAQ_weighted, center = T, scale = T)),
PAQ_scaled_target = c(scale(PAQ_target, center = TRUE, scale = TRUE)),
PAQ_scaled_actual = c(scale(PAQ_actual, center = TRUE, scale = TRUE)))
#regression_data %>% filter(is.na(PAQ_target))
session_data <- read_csv("Data/comparison_data_lyon.csv") %>%    # need to add ordinal session numbers for GAMMs
group_by(Speaker, age) %>%
tally() %>%
filter(n > 1) %>%
dplyr::select(Speaker, age) %>%
group_by(Speaker, age) %>%
tally() %>%
mutate(session_ordinal = row_number()) %>%
dplyr::select(-n)
freq_lyon <- read_csv("Data/freq_lyon.csv") %>%
mutate(Speaker = ifelse(Speaker == "Theotime", "Tim", Speaker))
# chi_freq_bychi <- read_csv("Data/chi_freq_bychi.csv")
# chi_freq_byword <- read_csv("Data/chi_freq_byword.csv")
word_cat <- feather::read_feather("Data/FULLsample_Lyon.feather") %>%
distinct(Gloss, .keep_all = T) %>%
dplyr::select(Gloss, category) %>%
rename("gloss1" = "Gloss") %>%
mutate(category = as.factor(category),
category = fct_collapse(category,
object_word = c("animals", "body_parts", "clothing", "food_drink", "furniture_rooms",
"household", "people", "outside", "places", "toys", "vehicles"),
verbs = c("action_words", "helping_verbs")))
FULLsample_var <- feather::read_feather("Data/FULLsample_Lyon.feather") %>%
group_by(Speaker, Gloss) %>%
tally() %>%
# left_join(word_cat) %>%
rename("gloss1" = "Gloss",
"n_tokens" = "n")   # how many tokens of each word included in the data
regression_data <- regression_data %>%
left_join(freq_lyon) %>%
left_join(word_cat) %>%
left_join(FULLsample_var) %>%
left_join(session_data) %>%
mutate(total_freq = ifelse(is.na(total_freq), 0, total_freq)) %>%
mutate(freq_scaled = c(scale(total_freq, center = TRUE, scale = TRUE)),
vocab_scaled = c(scale(vocab_agg, center = TRUE, scale = TRUE)),
tokens_scaled = c(scale(n_tokens, center = TRUE, scale = TRUE))) %>%
mutate(corpus = "French",
age_scaled = c(scale(age, center = T, scale = T)),
category = as.factor(category),
data_type = as.factor(data_type),
Speaker = as.factor(Speaker))
feather::write_feather(regression_data, "Data/regression_data_lyon.feather")
